{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_retrain.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJLDyfb4sA54",
        "outputId": "62d2a207-dc4d-43af-8581-3c7eb98efe39"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hVlHEE-sCCv",
        "outputId": "62753253-7c47-4678-ac06-d14e0d45818c"
      },
      "source": [
        "# Unzip here\n",
        "!unzip \"/content/drive/MyDrive/bert.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/bert.zip\n",
            "   creating: new_saved_model/bert/\n",
            "   creating: new_saved_model/bert/assets/\n",
            "  inflating: new_saved_model/bert/assets/vocab.txt  \n",
            "   creating: new_saved_model/bert/variables/\n",
            "  inflating: new_saved_model/bert/variables/variables.data-00000-of-00001  \n",
            "  inflating: new_saved_model/bert/variables/variables.index  \n",
            "  inflating: new_saved_model/bert/saved_model.pb  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aATHuVaWs4PS",
        "outputId": "0c6f02c9-e2a4-47bb-807f-098efdbe92e6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  new_saved_model\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex76WrQKtOlz"
      },
      "source": [
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ksXPemdseN2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official import nlp\n",
        "reloaded = tf.saved_model.load('new_saved_model/bert')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVtk-WzdtnJy",
        "outputId": "b4d795cc-ecd2-4762-f119-16debb89900e"
      },
      "source": [
        " model = tf.keras.models.load_model('new_saved_model/bert',compile=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7a8db69dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7a8db5cb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6k6PkA4v3yk"
      },
      "source": [
        "!mkdir text\n",
        "!mkdir text/clickbait\n",
        "!mkdir text/normal"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbph9AE0xsGE"
      },
      "source": [
        "githubUrl= 'https://raw.githubusercontent.com/mtp9k/Identifying-Clickbait-SYS-60616-/main/'\n",
        "full = f'{githubUrl}clickbait_data.csv'\n",
        "DATA_ENDPOINT = full"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOafMvvRvXH5",
        "outputId": "d08a5117-7a0d-4af9-e33e-5bd1b254f13b"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(DATA_ENDPOINT)\n",
        "df.to_csv('clickbait_train.csv',index=False)\n",
        "df = pd.read_csv('clickbait_train.csv')\n",
        "normal = df.loc[df['clickbait']==0]['headline'].values\n",
        "clickbait = df.loc[df['clickbait']==1]['headline'].values\n",
        "def generateTensorflowTextDir(data,folderName):\n",
        "    for i,line in enumerate(data):\n",
        "        filename = f'{i}_{folderName}'\n",
        "        with open(f\"text/{folderName}/{filename}.txt\", \"w\") as outfile:\n",
        "            outfile.write(line)\n",
        "generateTensorflowTextDir(normal,'normal')\n",
        "generateTensorflowTextDir(clickbait,'clickbait')\n",
        "# Batch Size -- we set to 1 to just read it in\n",
        "BATCH_SIZE = 64\n",
        "DATASET_SIZE = 32000\n",
        "train_size = 32000 - 6400\n",
        "test_size =6400\n",
        "seed = 49\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'text',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_names=['normal','clickbait'],\n",
        "    subset='training',\n",
        "    validation_split=0.2, \n",
        "    seed=seed\n",
        ")\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "   'text/',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_names=['normal','clickbait'],\n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 32000 files belonging to 2 classes.\n",
            "Using 25600 files for training.\n",
            "Found 32000 files belonging to 2 classes.\n",
            "Using 6400 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItjNVguDuqnQ"
      },
      "source": [
        "from tensorflow.keras import losses\n",
        "import tensorflow as tf\n",
        "from official import nlp\n",
        "import official.nlp.optimization\n",
        "loss=losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.1)\n",
        "epochs=3\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "steps_per_epoch = tf.data.experimental.cardinality(raw_train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "optimizer = nlp.optimization.create_optimizer(\n",
        "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "                        loss=loss,\n",
        "                        metrics=metrics)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EV9YeqnxK4Z"
      },
      "source": [
        "history = model.fit(\n",
        "    raw_train_ds,\n",
        "    validation_data=raw_val_ds,\n",
        "    epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9l-IuwOxZbL"
      },
      "source": [
        "export_dir='new_saved_model/bert'\n",
        "tf.saved_model.save(best_bert_model, export_dir=export_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agn0U7RZxc16"
      },
      "source": [
        "!zip -r new_saved_model/bert.zip new_saved_model/bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9bLLve1xeJ3"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"new_saved_model/bert.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}