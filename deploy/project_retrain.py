# -*- coding: utf-8 -*-
"""project_retrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Uxc0HW6lgiiU3dnLsV2Aa-pYbiVLGl9
"""

from google.colab import drive

drive.mount('/content/drive')

# Unzip here
!unzip "/content/drive/MyDrive/bert.zip"

!ls

!pip install -q tensorflow-text
!pip install -q tf-models-official

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official import nlp
reloaded = tf.saved_model.load('new_saved_model/bert')

model = tf.keras.models.load_model('new_saved_model/bert',compile=False)

!mkdir text
!mkdir text/clickbait
!mkdir text/normal

githubUrl= 'https://raw.githubusercontent.com/mtp9k/Identifying-Clickbait-SYS-60616-/main/'
full = f'{githubUrl}clickbait_data.csv'
DATA_ENDPOINT = full

import pandas as pd
df = pd.read_csv(DATA_ENDPOINT)
df.to_csv('clickbait_train.csv',index=False)
df = pd.read_csv('clickbait_train.csv')
normal = df.loc[df['clickbait']==0]['headline'].values
clickbait = df.loc[df['clickbait']==1]['headline'].values
def generateTensorflowTextDir(data,folderName):
    for i,line in enumerate(data):
        filename = f'{i}_{folderName}'
        with open(f"text/{folderName}/{filename}.txt", "w") as outfile:
            outfile.write(line)
generateTensorflowTextDir(normal,'normal')
generateTensorflowTextDir(clickbait,'clickbait')
# Batch Size -- we set to 1 to just read it in
BATCH_SIZE = 64
DATASET_SIZE = 32000
train_size = 32000 - 6400
test_size =6400
seed = 49
raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'text',
    batch_size=BATCH_SIZE,
    class_names=['normal','clickbait'],
    subset='training',
    validation_split=0.2, 
    seed=seed
)
raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(
   'text/',
    batch_size=BATCH_SIZE,
    class_names=['normal','clickbait'],
    validation_split=0.2, 
    subset='validation', 
    seed=seed)

from tensorflow.keras import losses
import tensorflow as tf
from official import nlp
import official.nlp.optimization
loss=losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.1)
epochs=3
metrics = tf.metrics.BinaryAccuracy()
steps_per_epoch = tf.data.experimental.cardinality(raw_train_ds).numpy()
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

optimizer = nlp.optimization.create_optimizer(
    2e-5, num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps)

model.compile(optimizer=optimizer,
                        loss=loss,
                        metrics=metrics)

history = model.fit(
    raw_train_ds,
    validation_data=raw_val_ds,
    epochs=epochs)

export_dir='new_saved_model/bert'
tf.saved_model.save(best_bert_model, export_dir=export_dir)

!zip -r new_saved_model/bert.zip new_saved_model/bert

from google.colab import files
files.download("new_saved_model/bert.zip")