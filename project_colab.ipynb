{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eastern-boulder",
        "insured-alert"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "compliant-exhaust"
      },
      "source": [
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras.layers import Embedding, LSTM\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "seed = 49"
      ],
      "id": "compliant-exhaust",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "photographic-trail"
      },
      "source": [
        "### Generating Text Files"
      ],
      "id": "photographic-trail"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "direct-caribbean"
      },
      "source": [
        "!rm -r text"
      ],
      "id": "direct-caribbean",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pregnant-power"
      },
      "source": [
        "!mkdir text\n",
        "!mkdir text/clickbait\n",
        "!mkdir text/normal"
      ],
      "id": "pregnant-power",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ViCVGsnQo9b"
      },
      "source": [
        "githubUrl= 'https://raw.githubusercontent.com/mtp9k/Identifying-Clickbait-SYS-60616-/main/'"
      ],
      "id": "-ViCVGsnQo9b",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dated-westminster"
      },
      "source": [
        "df = pd.read_csv(f'{githubUrl}clickbait_data.csv')"
      ],
      "id": "dated-westminster",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spectacular-cooperative"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.1,random_state=seed)"
      ],
      "id": "spectacular-cooperative",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cardiovascular-demographic"
      },
      "source": [
        "train.to_csv('clickbait_train.csv',index=False)\n",
        "test.to_csv('clickbait_test.csv',index=False)"
      ],
      "id": "cardiovascular-demographic",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "invisible-chile"
      },
      "source": [
        "df = pd.read_csv('clickbait_train.csv')"
      ],
      "id": "invisible-chile",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coated-speed"
      },
      "source": [
        "normal = df.loc[df['clickbait']==0]['headline'].values\n",
        "clickbait = df.loc[df['clickbait']==1]['headline'].values"
      ],
      "id": "coated-speed",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "conscious-listening"
      },
      "source": [
        "def generateTensorflowTextDir(data,folderName):\n",
        "    for i,line in enumerate(data):\n",
        "        filename = f'{i}_{folderName}'\n",
        "        with open(f\"text/{folderName}/{filename}.txt\", \"w\") as outfile:\n",
        "            outfile.write(line)"
      ],
      "id": "conscious-listening",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irish-pledge"
      },
      "source": [
        "generateTensorflowTextDir(normal,'normal')"
      ],
      "id": "irish-pledge",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musical-recipe"
      },
      "source": [
        "generateTensorflowTextDir(clickbait,'clickbait')"
      ],
      "id": "musical-recipe",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alive-sarah"
      },
      "source": [
        "### Generating Test Data"
      ],
      "id": "alive-sarah"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "regulation-belarus"
      },
      "source": [
        "testData = pd.read_csv(f'{githubUrl}clickbait_test.csv')\n",
        "testText = testData['headline'].values\n",
        "testLabels = testData['clickbait'].values"
      ],
      "id": "regulation-belarus",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "outer-north"
      },
      "source": [
        "tweetTestData = pd.read_csv(f'{githubUrl}tweets.csv')\n",
        "tweetTestText = [' '.join(map(lambda x: x.strip(\"\\n;[]\\\\\"), l.split(' '))) for l in tweetTestData.postText]\n",
        "\n",
        "tweetTestLabels = np.round(tweetTestData['isClickbait'].values)"
      ],
      "id": "outer-north",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "according-segment"
      },
      "source": [
        "### Evaluation Helper functions"
      ],
      "id": "according-segment"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "colonial-metropolitan"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def getAccuracy(labels,predictions):\n",
        "    return accuracy_score(labels,np.round(predictions))"
      ],
      "id": "colonial-metropolitan",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bibliographic-wholesale"
      },
      "source": [
        "def export_model(model):\n",
        "    ouput = tf.keras.Sequential([\n",
        "      vectorize_layer,\n",
        "      model,\n",
        "    ])\n",
        "    return ouput"
      ],
      "id": "bibliographic-wholesale",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "backed-leader"
      },
      "source": [
        "def runEvaluation(model,returnPredictions=False):\n",
        "    model_predictions= model.predict(testText)\n",
        "    model_accuracy = getAccuracy(testLabels,model_predictions)\n",
        "    model_tweet_predictions = model.predict(tweetTestText)\n",
        "    model_tweet_accuracy = getAccuracy(tweetTestLabels,model_tweet_predictions)\n",
        "    if returnPredictions:\n",
        "      return model_predictions,model_tweet_predictions\n",
        "    return model_accuracy,model_tweet_accuracy\n",
        "    "
      ],
      "id": "backed-leader",
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unique-scholarship"
      },
      "source": [
        "### Parsing Text Files"
      ],
      "id": "unique-scholarship"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elect-airport"
      },
      "source": [
        "# Batch Size -- we set to 1 to just read it in\n",
        "BATCH_SIZE = 64\n",
        "DATASET_SIZE = 32000\n",
        "train_size = 32000 - 6400\n",
        "test_size =6400\n",
        "seed = 49"
      ],
      "id": "elect-airport",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "copyrighted-retail",
        "outputId": "c6c30e54-001b-4d76-a3a1-76e72970754d"
      },
      "source": [
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'text',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_names=['normal','clickbait'],\n",
        "    subset='training',\n",
        "    validation_split=0.2, \n",
        "    seed=seed\n",
        ")"
      ],
      "id": "copyrighted-retail",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28800 files belonging to 2 classes.\n",
            "Using 23040 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pleasant-soviet",
        "outputId": "e7e7c5a8-a771-4083-f1bc-f2de2dd24703"
      },
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ],
      "id": "pleasant-soviet",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 0 corresponds to normal\n",
            "Label 1 corresponds to clickbait\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "billion-roads",
        "outputId": "a89f7270-b364-40fa-f3d8-d7b70892c73c"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "   'text/',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_names=['normal','clickbait'],\n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ],
      "id": "billion-roads",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28800 files belonging to 2 classes.\n",
            "Using 5760 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "official-graduation",
        "outputId": "2b9761fb-b703-4c5f-aea2-60ec8fa1e8fa"
      },
      "source": [
        "for x,y in raw_train_ds.take(1):\n",
        "    print(x[0:10],y[0:10])"
      ],
      "id": "official-graduation",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'13 Ways Your Body Can Go Wrong In Space'\n",
            " b\"Blizzard Entertainment's victory over bnetd sealed in Appeals Court\"\n",
            " b'Tankers taken by pirates in Indian Ocean'\n",
            " b'Sorority Girls On A Selfie Spree At A Baseball Game Made People Really Mad For Some Reason'\n",
            " b\"US government 'can continue' eavesdropping\"\n",
            " b'19 Texts All Twentysomethings Have Sent Their Dad'\n",
            " b'Leipheimer Drops in Giro, but It Could Have Been Worse'\n",
            " b\"If Celebrities' Instagrams Were On Myspace\"\n",
            " b'Zimbabwe bans crop growing in urban areas'\n",
            " b'You Will Never Use Chopsticks A Different Way Again'], shape=(10,), dtype=string) tf.Tensor([1 0 0 1 0 1 0 1 0 1], shape=(10,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "applicable-likelihood"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    return input_data\n",
        "    return lowercase"
      ],
      "id": "applicable-likelihood",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boxed-belfast"
      },
      "source": [
        "max_features = 5000\n",
        "sequence_length = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "#     standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "id": "boxed-belfast",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "valuable-wichita"
      },
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "id": "valuable-wichita",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reasonable-juvenile"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "    #text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label"
      ],
      "id": "reasonable-juvenile",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virtual-anatomy"
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)"
      ],
      "id": "virtual-anatomy",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "infinite-immigration"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "id": "infinite-immigration",
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compatible-theory"
      },
      "source": [
        "embedding_dim = 32"
      ],
      "id": "compatible-theory",
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "artistic-spokesman"
      },
      "source": [
        "## Experiment 1: Simple NN Performance vs Traditional ML"
      ],
      "id": "artistic-spokesman"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eastern-boulder"
      },
      "source": [
        "### Base ML Models"
      ],
      "id": "eastern-boulder"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "industrial-perception"
      },
      "source": [
        "def dsToNumpy(ds):\n",
        "    xList=[]\n",
        "    yList=[]\n",
        "    for x,y in ds.unbatch():\n",
        "        xList.append(x.numpy())\n",
        "        yList.append(y.numpy())\n",
        "    xList=np.array(xList)\n",
        "    yList=np.array(yList)\n",
        "    return xList,yList\n",
        "x_train,y_train = dsToNumpy(train_ds)\n",
        "x_val,y_val = dsToNumpy(val_ds)"
      ],
      "id": "industrial-perception",
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cardiac-panel",
        "outputId": "ee07eac7-7049-45ae-f997-1e6881f58901"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(max_depth=10, random_state=seed).fit(x_train, y_train)\n",
        "predictions=rf.predict(x_val)\n",
        "predictions"
      ],
      "id": "cardiac-panel",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juvenile-huntington",
        "outputId": "3744a7cc-2bc0-4e02-f53e-485daa0d820b"
      },
      "source": [
        "getAccuracy(y_val,predictions)"
      ],
      "id": "juvenile-huntington",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8274305555555556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "insured-alert"
      },
      "source": [
        "### Simple LSTM Model"
      ],
      "id": "insured-alert"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "changed-honey"
      },
      "source": [
        "def LSTM_nn():\n",
        "    model = tf.keras.Sequential([\n",
        "      layers.Embedding(max_features, embedding_dim,input_length=sequence_length),\n",
        "      LSTM(32, return_sequences=True),\n",
        "      layers.GlobalAveragePooling1D(),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(1,activation='sigmoid')])\n",
        "    return model"
      ],
      "id": "changed-honey",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coral-haiti",
        "outputId": "f6353771-91a0-4ff8-fe44-e85abd2818f2"
      },
      "source": [
        "lstm_model = LSTM_nn()\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "lstm_model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "epochs = 5\n",
        "history = lstm_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "id": "coral-haiti",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "360/360 [==============================] - 11s 26ms/step - loss: 0.6378 - accuracy: 0.5639 - val_loss: 0.2240 - val_accuracy: 0.9436\n",
            "Epoch 2/5\n",
            "360/360 [==============================] - 9s 25ms/step - loss: 0.2039 - accuracy: 0.9487 - val_loss: 0.1745 - val_accuracy: 0.9578\n",
            "Epoch 3/5\n",
            "360/360 [==============================] - 9s 25ms/step - loss: 0.1502 - accuracy: 0.9648 - val_loss: 0.1587 - val_accuracy: 0.9637\n",
            "Epoch 4/5\n",
            "360/360 [==============================] - 9s 25ms/step - loss: 0.1178 - accuracy: 0.9754 - val_loss: 0.1515 - val_accuracy: 0.9661\n",
            "Epoch 5/5\n",
            "360/360 [==============================] - 9s 25ms/step - loss: 0.0916 - accuracy: 0.9818 - val_loss: 0.1568 - val_accuracy: 0.9653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clean-guarantee"
      },
      "source": [
        "export_lstm_model = export_model(lstm_model)\n",
        "lstm_accuracy,lstm_tweet_accuracy = runEvaluation(export_lstm_model)"
      ],
      "id": "clean-guarantee",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flying-medium",
        "outputId": "67e87726-601d-4a5a-c6f5-c1849b8acf8e"
      },
      "source": [
        "print('same dataset evaluation accuracy:')\n",
        "print(lstm_accuracy)\n",
        "print('twitter dataset evaluation accuracy:')\n",
        "print(lstm_tweet_accuracy)"
      ],
      "id": "flying-medium",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "same dataset evaluation accuracy:\n",
            "0.971875\n",
            "twitter dataset evaluation accuracy:\n",
            "0.5203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "increased-checkout"
      },
      "source": [
        "## Experiment 2: NN Architecture Comparisons + Ensemble"
      ],
      "id": "increased-checkout"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "greek-clark"
      },
      "source": [
        "### Simple GRU Model"
      ],
      "id": "greek-clark"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fossil-northwest"
      },
      "source": [
        "def GRU_nn():\n",
        "    model = tf.keras.Sequential([\n",
        "      layers.Embedding(max_features, embedding_dim,input_length=sequence_length),\n",
        "      layers.GRU(32, return_sequences=True),\n",
        "      layers.GlobalAveragePooling1D(),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(1,activation='sigmoid')])\n",
        "    return model"
      ],
      "id": "fossil-northwest",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "varied-document",
        "outputId": "62b4c667-2f25-426a-c1cc-2a1eca355552"
      },
      "source": [
        "gru_model = GRU_nn()\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "gru_model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "epochs = 5\n",
        "history = gru_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "id": "varied-document",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "360/360 [==============================] - 10s 25ms/step - loss: 0.6556 - accuracy: 0.5594 - val_loss: 0.1700 - val_accuracy: 0.9554\n",
            "Epoch 2/5\n",
            "360/360 [==============================] - 9s 24ms/step - loss: 0.1709 - accuracy: 0.9528 - val_loss: 0.1268 - val_accuracy: 0.9651\n",
            "Epoch 3/5\n",
            "360/360 [==============================] - 9s 24ms/step - loss: 0.1060 - accuracy: 0.9721 - val_loss: 0.1075 - val_accuracy: 0.9665\n",
            "Epoch 4/5\n",
            "360/360 [==============================] - 9s 24ms/step - loss: 0.0727 - accuracy: 0.9816 - val_loss: 0.1006 - val_accuracy: 0.9707\n",
            "Epoch 5/5\n",
            "360/360 [==============================] - 9s 24ms/step - loss: 0.0612 - accuracy: 0.9837 - val_loss: 0.1069 - val_accuracy: 0.9708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coordinate-affiliation"
      },
      "source": [
        "export_gru_model = export_model(gru_model)\n",
        "gru_accuracy,gru_tweet_accuracy = runEvaluation(export_gru_model)"
      ],
      "id": "coordinate-affiliation",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fundamental-superior",
        "outputId": "f81fb592-a660-47c5-da77-3e9afd55230e"
      },
      "source": [
        "print('same dataset evaluation accuracy:')\n",
        "print(gru_accuracy)\n",
        "print('twitter dataset evaluation accuracy:')\n",
        "print(gru_tweet_accuracy)"
      ],
      "id": "fundamental-superior",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "same dataset evaluation accuracy:\n",
            "0.975625\n",
            "twitter dataset evaluation accuracy:\n",
            "0.5343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "billion-deposit"
      },
      "source": [
        "### BERT Model"
      ],
      "id": "billion-deposit"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZWZoA62ShyD",
        "outputId": "6e770245-f703-4c36-dda2-2bf40258c706"
      },
      "source": [
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official"
      ],
      "id": "rZWZoA62ShyD",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4MB 7.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 25.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 706kB 36.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 56.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 35.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 134kB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 26.5MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ongoing-gothic"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optmizer"
      ],
      "id": "ongoing-gothic",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guilty-sierra",
        "outputId": "eccf7dce-05a1-4d85-f66a-00e8a56081c0"
      },
      "source": [
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "id": "guilty-sierra",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "standard-philosophy"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "text_test = ['this is such an amazing movie!']\n",
        "text_preprocessed = bert_preprocess_model(text_test)"
      ],
      "id": "standard-philosophy",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incorporated-chocolate"
      },
      "source": [
        "def BERT_nn():\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "    outputs = encoder(encoder_inputs)\n",
        "    net = outputs['pooled_output']\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "    net = tf.keras.layers.Dense(1,activation='sigmoid')(net)\n",
        "    return tf.keras.Model(text_input, net)"
      ],
      "id": "incorporated-chocolate",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "solid-article",
        "outputId": "f5bfb0c0-93da-4477-c1f1-98eecc933ab1"
      },
      "source": [
        "bert_model = BERT_nn()\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "epochs = 2\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "bert_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "history = bert_model.fit(\n",
        "    raw_train_ds,\n",
        "    validation_data=raw_val_ds,\n",
        "    epochs=epochs)"
      ],
      "id": "solid-article",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f38773985f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f38773985f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3c9a344ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3c9a344ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3cab1845f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3cab1845f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "360/360 [==============================] - 146s 388ms/step - loss: 0.1841 - binary_accuracy: 0.9262 - val_loss: 0.0887 - val_binary_accuracy: 0.9675\n",
            "Epoch 2/2\n",
            "360/360 [==============================] - 140s 388ms/step - loss: 0.0899 - binary_accuracy: 0.9675 - val_loss: 0.0887 - val_binary_accuracy: 0.9675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO0o3Jk9W4ms"
      },
      "source": [
        "bert_accuracy,bert_tweet_accuracy = runEvaluation(bert_model)"
      ],
      "id": "bO0o3Jk9W4ms",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi4Dhn8TXNSW",
        "outputId": "6dce3c2c-e841-4300-dfcc-3f4699dbc092"
      },
      "source": [
        "print('same dataset evaluation accuracy:')\n",
        "print(bert_accuracy)\n",
        "print('twitter dataset evaluation accuracy:')\n",
        "print(bert_tweet_accuracy)"
      ],
      "id": "pi4Dhn8TXNSW",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "same dataset evaluation accuracy:\n",
            "0.9728125\n",
            "twitter dataset evaluation accuracy:\n",
            "0.4544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2T0_3GcZNSw"
      },
      "source": [
        "### Results"
      ],
      "id": "G2T0_3GcZNSw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "u8EmLBHbZPQh",
        "outputId": "a007e249-f6c4-4c27-e0b3-f577fb7cd7ce"
      },
      "source": [
        "results = pd.DataFrame({'Model':['LSTM','GRU','BERT'],'Test Set Accuracy':[lstm_accuracy,gru_accuracy,bert_accuracy],'Twitter Set Accuracy':[lstm_tweet_accuracy,gru_tweet_accuracy,bert_tweet_accuracy]})\n",
        "results"
      ],
      "id": "u8EmLBHbZPQh",
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Test Set Accuracy</th>\n",
              "      <th>Twitter Set Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>0.971875</td>\n",
              "      <td>0.5203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU</td>\n",
              "      <td>0.975625</td>\n",
              "      <td>0.5343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BERT</td>\n",
              "      <td>0.972812</td>\n",
              "      <td>0.4544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Model  Test Set Accuracy  Twitter Set Accuracy\n",
              "0  LSTM           0.971875                0.5203\n",
              "1   GRU           0.975625                0.5343\n",
              "2  BERT           0.972812                0.4544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5eMOS7xaJNv"
      },
      "source": [
        "### Naieve Ensemble"
      ],
      "id": "b5eMOS7xaJNv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BajR_SvadJ3"
      },
      "source": [
        "lstm_pred,lstm_tweet_pred = runEvaluation(export_lstm_model,True)\n",
        "gru_pred,gru_tweet_pred = runEvaluation(export_gru_model,True)\n",
        "bert_pred,bert_tweet_pred = runEvaluation(bert_model,True)"
      ],
      "id": "1BajR_SvadJ3",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBaSSWUwavyB",
        "outputId": "f5b10941-acfe-4e9e-8067-9ae8ef2d5668"
      },
      "source": [
        "ensemble_predictions = (lstm_pred + gru_pred + bert_pred) / 3\n",
        "ensemble_tweet_predictions = (lstm_tweet_pred + gru_tweet_pred + bert_tweet_pred) / 3\n",
        "ensemble_acc = getAccuracy(testLabels,ensemble_predictions)\n",
        "ensemble_tweet_acc = getAccuracy(tweetTestLabels,ensemble_tweet_predictions)\n",
        "print('Ensemble Test Set Accuracy:')\n",
        "print(ensemble_acc)\n",
        "print('Ensemble Tweet Set Accuracy:')\n",
        "print(ensemble_tweet_acc)"
      ],
      "id": "fBaSSWUwavyB",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ensemble Test Set Accuracy:\n",
            "0.97875\n",
            "Ensemble Tweet Set Accuracy:\n",
            "0.5187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "streaming-secondary"
      },
      "source": [
        "## Experiment 3: Performance of Label Smoothing"
      ],
      "id": "streaming-secondary"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "linear-trance",
        "outputId": "a3f59384-4ee2-49e9-bb56-99cf2f218cea"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "smooth_model = GRU_nn()\n",
        "label_smoothing =  1\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "smooth_model.compile(loss=losses.BinaryCrossentropy(from_logits=True,label_smoothing=label_smoothing),\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "epochs = 5\n",
        "history = smooth_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "id": "linear-trance",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "360/360 [==============================] - 10s 25ms/step - loss: 0.6932 - accuracy: 0.5098 - val_loss: 0.6931 - val_accuracy: 0.5311\n",
            "Epoch 2/5\n",
            "360/360 [==============================] - 9s 24ms/step - loss: 0.6931 - accuracy: 0.5177 - val_loss: 0.6931 - val_accuracy: 0.5248\n",
            "Epoch 3/5\n",
            "360/360 [==============================] - 9s 24ms/step - loss: 0.6931 - accuracy: 0.5265 - val_loss: 0.6931 - val_accuracy: 0.5198\n",
            "Epoch 4/5\n",
            "360/360 [==============================] - 9s 24ms/step - loss: 0.6931 - accuracy: 0.5142 - val_loss: 0.6931 - val_accuracy: 0.5151\n",
            "Epoch 5/5\n",
            "360/360 [==============================] - 9s 24ms/step - loss: 0.6931 - accuracy: 0.5108 - val_loss: 0.6931 - val_accuracy: 0.5141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSytQOnUb0z9"
      },
      "source": [
        "export_smooth_model = export_model(smooth_model)\n",
        "smooth_accuracy,smooth_tweet_accuracy = runEvaluation(export_smooth_model)"
      ],
      "id": "WSytQOnUb0z9",
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgaAqwVMb65I",
        "outputId": "003bca37-c4b9-4b51-a519-d9d2d5c21e33"
      },
      "source": [
        "print('same dataset evaluation accuracy:')\n",
        "print(smooth_accuracy)\n",
        "print('twitter dataset evaluation accuracy:')\n",
        "print(smooth_tweet_accuracy)"
      ],
      "id": "XgaAqwVMb65I",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "same dataset evaluation accuracy:\n",
            "0.9790625\n",
            "twitter dataset evaluation accuracy:\n",
            "0.5327\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}